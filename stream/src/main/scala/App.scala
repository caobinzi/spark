import scala.language.higherKinds
import scala.language.implicitConversions
import scala.util.Try
import scalaz._
import scalaz.Scalaz._
import concurrent._
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SQLContext
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext, Time}
import org.apache.spark.sql.DataFrame
import org.apache.spark.streaming.dstream.DStream

/**
 * Use DataFrames and SQL to count words in UTF8 encoded, '\n' delimited text received from the
 * network every second.
 *
 * Usage: SqlNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.SqlNetworkWordCount localhost 9999`
 */

object SqlNetworkWordCount extends App {

  // Create the context with a 2 second batch size
  val sparkConf = new SparkConf().setAppName("SqlNetworkWordCount").setMaster("local[4]")
  val ssc = new StreamingContext(sparkConf, Seconds(2))

  // Create a socket stream on target ip:port and count the
  // words in input stream of \n delimited text (eg. generated by 'nc')
  // Note that no duplication in storage level only for running locally.
  // Replication necessary in distributed scenario for fault tolerance.

  val lines = ssc.socketTextStream("localhost", 9998, StorageLevel.MEMORY_AND_DISK_SER)


  val words: DStream[String] = lines.flatMap{
    x ⇒
      //println("Got --->" + x)
      x.split(" ")
  }
  process_with_sql(words)

  ssc.start()
  ssc.awaitTermination()

  def process(words: DStream[String]) = {
    val pairs = words.map(word ⇒ (word, 1))
    val wordCounts = pairs.reduceByKey(_ + _)

    wordCounts.print()
  }

  def process_with_sql(words: DStream[String]) = {
    // Convert RDDs of the words DStream to DataFrame and run SQL query


    words.foreachRDD((rdd: RDD[String], time: Time) ⇒ {
      // Get the singleton instance of SQLContext
      val sqlContext = SQLContextSingleton.getInstance(rdd.sparkContext)
      import sqlContext.implicits._

      // Convert RDD[String] to RDD[case class] to DataFrame

      val wordsDataFrame: DataFrame = rdd.map(
        w ⇒ Record(w, w.length, w.length+1)
      ).toDF()

      // Register as table
      wordsDataFrame.registerTempTable("words")
      println("Words Data Frame Length = "+wordsDataFrame.count())
      wordsDataFrame.foreach(println)

      // Do word count on table using SQL and print it
      val wordCountsDataFrame =
      sqlContext.sql("select word, count(*) as total from words where b > 3 group by word")
      println(s"========= $time =========")
      wordCountsDataFrame.show()
    })

  }
}

/** Case class for converting RDD to DataFrame */
case class Record(word: String, b:Int, c:Int)

/** Lazily instantiated singleton instance of SQLContext */
object SQLContextSingleton {

  @transient private var instance: SQLContext = _
  def getInstance(sparkContext: SparkContext): SQLContext = {
    if (instance == null) {
      instance = new SQLContext(sparkContext)
    }
    instance
  }
}

